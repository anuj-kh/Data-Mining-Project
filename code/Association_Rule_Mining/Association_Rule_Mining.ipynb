{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code will take about 1-2 minutes to compute the association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Generator function of all combinations based on the last state of Apriori algorithm\n",
    "#    Parameters\n",
    "#    -----------\n",
    "#    old_combinations: np.array\n",
    "#        All combinations with enough support in the last step\n",
    "#        Combinations are represented by a matrix.\n",
    "#        Number of columns is equal to the combination size\n",
    "#        of the previous step.\n",
    "#        Each row represents one combination\n",
    "#        and contains item type ids in the ascending order\n",
    "#        \n",
    "#    Returns\n",
    "#    -----------\n",
    "#    Generator of all combinations from the last step x items\n",
    "#    from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_combinations(old_combinations):\n",
    "    \n",
    "    items_types_in_previous_step = np.unique(old_combinations.flatten())\n",
    "    for old_combination in old_combinations:\n",
    "        max_combination = old_combination[-1]\n",
    "        mask = items_types_in_previous_step > max_combination\n",
    "        valid_items = items_types_in_previous_step[mask]\n",
    "        old_tuple = tuple(old_combination)\n",
    "        for item in valid_items:\n",
    "            yield from old_tuple\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Function to get frequent itemsets from a one-hot DataFrame\n",
    "#    Parameters\n",
    "#    -----------\n",
    "#    df : pandas DataFrame\n",
    "#       The DataFrame in a one-hot encoded format \n",
    "#       from which frequent itemsets will be mined\n",
    "#      \n",
    "#    min_support : float\n",
    "#       A float between 0 and 1 for minumum support of the itemsets returned.\n",
    "#       The support is computed as the fraction\n",
    "#       `transactions_where_item(s)_occur / total_transactions`.\n",
    "#\n",
    "#    use_colnames : bool (default: False)\n",
    "#       If `True`, uses the DataFrames' column names in the returned DataFrame\n",
    "#       instead of column indices.\n",
    "#\n",
    "#    Returns\n",
    "#    -----------\n",
    "#    pandas DataFrame with columns ['support', 'itemsets'] of all itemsets\n",
    "#      that are >= `min_support`\n",
    "#      Each itemset in the 'itemsets' column is of type `frozenset`,\n",
    "#      which is a Python built-in type that behaves similarly to\n",
    "#      sets except that it is immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(df, min_support, use_colnames=False):\n",
    "    \n",
    "    #    A private function to calculate support as the\n",
    "    #    row-wise sum of values / number of rows\n",
    "    #    Parameters\n",
    "    #    -----------\n",
    "    #      _x : matrix of bools or binary\n",
    "    #      _n_rows : numeric, number of rows in _x\n",
    "    #      _is_sparse : bool True if _x is sparse\n",
    "    \n",
    "    #   Returns\n",
    "    #   -----------\n",
    "    #      np.array, shape = (n_rows, )\n",
    "    def _support(_x, _n_rows):\n",
    "        out = (np.sum(_x, axis=0) / _n_rows)\n",
    "        return np.array(out)\n",
    "\n",
    "    X = df.values\n",
    "  \n",
    "    support = _support(X, X.shape[0])\n",
    "    ary_col_idx = np.arange(X.shape[1])\n",
    "    support_dict = {1: support[support >= min_support]}\n",
    "    itemset_dict = {1: ary_col_idx[support >= min_support].reshape(-1, 1)}\n",
    "    max_itemset = 1\n",
    "    rows_count = float(X.shape[0])\n",
    "\n",
    "    while max_itemset and max_itemset < (float('inf')):\n",
    "        next_max_itemset = max_itemset + 1\n",
    "        \n",
    "        combin = generate_new_combinations(itemset_dict[max_itemset])\n",
    "        combin = np.fromiter(combin, dtype=int)\n",
    "        combin = combin.reshape(-1, next_max_itemset)\n",
    "\n",
    "        if combin.size == 0:\n",
    "            break\n",
    "\n",
    "        _bools = np.all(X[:, combin], axis=2)\n",
    "\n",
    "        support = _support(np.array(_bools), rows_count)\n",
    "        _mask = (support >= min_support).reshape(-1)\n",
    "        if any(_mask):\n",
    "            itemset_dict[next_max_itemset] = np.array(combin[_mask])\n",
    "            support_dict[next_max_itemset] = np.array(support[_mask])\n",
    "            max_itemset = next_max_itemset\n",
    "        else:\n",
    "            # Exit condition\n",
    "            break\n",
    "\n",
    "    all_res = []\n",
    "    for k in sorted(itemset_dict):\n",
    "        support = pd.Series(support_dict[k])\n",
    "        itemsets = pd.Series([frozenset(i) for i in itemset_dict[k]],\n",
    "                             dtype='object')\n",
    "\n",
    "        res = pd.concat((support, itemsets), axis=1)\n",
    "        all_res.append(res)\n",
    "\n",
    "    res_df = pd.concat(all_res)\n",
    "    res_df.columns = ['support', 'itemsets']\n",
    "    if use_colnames:\n",
    "        mapping = {idx: item for idx, item in enumerate(df.columns)}\n",
    "        res_df['itemsets'] = res_df['itemsets'].apply(lambda x: frozenset([\n",
    "                                                      mapping[i] for i in x]))\n",
    "    res_df = res_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Function to generate a DataFrame of association rules including the\n",
    "#    metrics 'support', 'confidence', and 'lift'\n",
    "#    Parameters\n",
    "#    -----------\n",
    "#    df : pandas DataFrame\n",
    "#      pandas DataFrame of frequent itemsets\n",
    "#      with columns ['support', 'itemsets']\n",
    "\n",
    "#    metric : string (default: 'confidence')\n",
    "      \n",
    "#    min_threshold : float (default: 0.8)\n",
    "#      Minimal threshold for the evaluation metric,\n",
    "#      via the `metric` parameter,\n",
    "#      to decide whether a candidate rule is of interest.\n",
    "\n",
    "#    Returns\n",
    "#    ----------\n",
    "#    pandas DataFrame with columns \"antecedents\" and \"consequents\"\n",
    "#      that store itemsets, plus the scoring metric columns:\n",
    "#      \"antecedent support\", \"consequent support\",\n",
    "#      \"support\", \"confidence\", \"lift\",\n",
    "#      of all rules for which\n",
    "#      metric(rule) >= min_threshold.\n",
    "#      Each entry in the \"antecedents\" and \"consequents\" columns are\n",
    "#      of type `frozenset`, which is a Python built-in type that\n",
    "#      behaves similarly to sets except that it is immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rules(df, metric=\"confidence\", min_threshold=0.8):\n",
    "    \n",
    "    # metrics for association rules\n",
    "    metric_dict = {\n",
    "        \"antecedent support\": lambda _, sA, __: sA,\n",
    "        \"consequent support\": lambda _, __, sC: sC,\n",
    "        \"support\": lambda sAC, _, __: sAC,\n",
    "        \"confidence\": lambda sAC, sA, _: sAC/sA,\n",
    "        \"lift\": lambda sAC, sA, sC: metric_dict[\"confidence\"](sAC, sA, sC)/sC,\n",
    "        }\n",
    "\n",
    "    columns_ordered = [\"antecedent support\", \"consequent support\",\n",
    "                       \"support\",\n",
    "                       \"confidence\", \"lift\",]\n",
    "\n",
    "\n",
    "    # get dict of {frequent itemset} -> support\n",
    "    keys = df['itemsets'].values\n",
    "    values = df['support'].values\n",
    "    frozenset_vect = np.vectorize(lambda x: frozenset(x))\n",
    "    frequent_items_dict = dict(zip(frozenset_vect(keys), values))\n",
    "\n",
    "    # prepare buckets to collect frequent rules\n",
    "    rule_antecedents = []\n",
    "    rule_consequents = []\n",
    "    rule_supports = []\n",
    "\n",
    "    # iterate over all frequent itemsets\n",
    "    for k in frequent_items_dict.keys():\n",
    "        sAC = frequent_items_dict[k]\n",
    "        # to find all possible combinations\n",
    "        for idx in range(len(k)-1, 0, -1):\n",
    "            # of antecedent and consequent\n",
    "            for c in combinations(k, r=idx):\n",
    "                antecedent = frozenset(c)\n",
    "                consequent = k.difference(antecedent)\n",
    "\n",
    "                try:\n",
    "                    sA = frequent_items_dict[antecedent]\n",
    "                    sC = frequent_items_dict[consequent]\n",
    "                except KeyError as e:\n",
    "                    s = (str(e) + 'You are likely getting this error'\n",
    "                                  ' because the DataFrame is missing '\n",
    "                                  ' antecedent and/or consequent '\n",
    "                                  ' information.')\n",
    "                    raise KeyError(s)\n",
    "                # check for the threshold\n",
    "\n",
    "                score = metric_dict[metric](sAC, sA, sC)\n",
    "                if score >= min_threshold:\n",
    "                    rule_antecedents.append(antecedent)\n",
    "                    rule_consequents.append(consequent)\n",
    "                    rule_supports.append([sAC, sA, sC])\n",
    "\n",
    "    # check if frequent rule was generated\n",
    "    if not rule_supports:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"antecedents\", \"consequents\"] + columns_ordered)\n",
    "\n",
    "    else:\n",
    "        # generate metrics\n",
    "        rule_supports = np.array(rule_supports).T.astype(float)\n",
    "        df_res = pd.DataFrame(\n",
    "            data=list(zip(rule_antecedents, rule_consequents)),\n",
    "            columns=[\"antecedents\", \"consequents\"])\n",
    "\n",
    "        \n",
    "        sAC = rule_supports[0]\n",
    "        sA = rule_supports[1]\n",
    "        sC = rule_supports[2]\n",
    "        for m in columns_ordered:\n",
    "            df_res[m] = metric_dict[m](sAC, sA, sC)\n",
    "\n",
    "        return df_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset of all the 12 months and concatenating them into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/April 2018/Cleaned_Data_Set_April_2018.csv')\n",
    "df2 = pd.read_csv('../../data/May 2018/Cleaned_Data_Set_May_2018.csv')\n",
    "df = df.append(df2, ignore_index='True')\n",
    "\n",
    "df2 = pd.read_csv('../../data/June 2018/Cleaned_Data_Set_June_2018.csv')\n",
    "df = df.append(df2, ignore_index='True')\n",
    "\n",
    "df2 = pd.read_csv('../../data/July 2018/Cleaned_Data_Set_July_2018.csv')\n",
    "df = df.append(df2, ignore_index='True')\n",
    "\n",
    "df2 = pd.read_csv('../../data/August 2018/Cleaned_Data_Set_August_2018.csv')\n",
    "df = df.append(df2, ignore_index='True')\n",
    "\n",
    "df2 = pd.read_csv('../../data/September 2018/Cleaned_Data_Set_September_2018.csv')\n",
    "df = df.append(df2, ignore_index='True')\n",
    "\n",
    "df2 = pd.read_csv('../../data/October 2018/Cleaned_Data_Set_October_2018.csv')\n",
    "df = df.append(df2, ignore_index='True')\n",
    "\n",
    "df2 = pd.read_csv('../../data/November 2018/Cleaned_Data_Set_November_2018.csv')\n",
    "df = df.append(df2, ignore_index='True')\n",
    "\n",
    "df2 = pd.read_csv('../../data/December 2018/Cleaned_Data_Set_December_2018.csv')\n",
    "df = df.append(df2, ignore_index='True')\n",
    "\n",
    "df2 = pd.read_csv('../../data/January 2019/Cleaned_Data_Set_January_2019.csv')\n",
    "df = df.append(df2, ignore_index='True')\n",
    "\n",
    "df2 = pd.read_csv('../../data/February 2019/Cleaned_Data_Set_February_2019.csv')\n",
    "df = df.append(df2, ignore_index='True')\n",
    "\n",
    "df2 = pd.read_csv('../../data/March 2019/Cleaned_Data_Set_March_2019.csv')\n",
    "df = df.append(df2, ignore_index='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns which provide no practical use to the process (Latitude, Longitude)\n",
    "# and creating a list of all unique items in the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n",
    "df = df.rename(columns={\"Network Type\": \"Network\", \"State Name\": \"State\", \"In Out Travelling\": \"In_Out_Travelling\", \"Call Drop Category\": \"Call_Drop_Category\" })\n",
    "df.Rating.value_counts()\n",
    "\n",
    "items1 = (df.Operator.unique())\n",
    "items2 = df.Network.unique()\n",
    "items3 = df.Rating.unique()\n",
    "items4 = df.State.unique()\n",
    "items5 = df.Call_Drop_Category.unique()\n",
    "items6 = df.In_Out_Travelling.unique()\n",
    "\n",
    "items = np.concatenate((items1,items2,items3,items4,items5,items6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the DataFrame into the One-Hot Encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_vals = []\n",
    "for index, row in df.iterrows():\n",
    "    labels = {}\n",
    "    uncommons = list(set(items) - set(row))\n",
    "    commons = list(set(items).intersection(row))\n",
    "    for uc in uncommons:\n",
    "        labels[uc] = 0\n",
    "    for com in commons:\n",
    "        labels[com] = 1\n",
    "    encoded_vals.append(labels)\n",
    "    \n",
    "ohe_df = pd.DataFrame(encoded_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the frequent itemsets with minimum support = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_items = apriori(ohe_df, min_support=0.04, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining the association rules with minimum confidence = 0.8 and lift > 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(freq_items, metric=\"confidence\", min_threshold=0.8)\n",
    "rules_final = rules[rules[\"lift\"] > 1.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the final results into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Association Rule Mining Result.txt\", \"w\")\n",
    "i=1\n",
    "for ind in rules_final.index :\n",
    "    l1 = list(rules_final['antecedents'][ind])\n",
    "    l2 = list(rules_final['consequents'][ind])\n",
    "    \n",
    "    str1 = '['        \n",
    "    for ele in l1[:-1]:\n",
    "        ele = str(ele)\n",
    "        str1 += ele\n",
    "        str1 += ', '\n",
    "    \n",
    "    str1 += str(l1[-1])\n",
    "    str1 += ']'\n",
    "    \n",
    "    str2 = '['        \n",
    "    for ele in l2[:-1]:\n",
    "        ele = str(ele)\n",
    "        str2 += ele\n",
    "        str2 += ', '\n",
    "    \n",
    "    str2 += str(l2[-1])\n",
    "    str2 += ']'\n",
    "    \n",
    "    supp = str(rules_final['support'][ind])\n",
    "    conf = str(rules_final['confidence'][ind])\n",
    "    lift = str(rules_final['lift'][ind])\n",
    "    \n",
    "    fstr = str(i) +'. ' + str1 + ' -> ' + str2 + '\\n' + 'Support : ' + supp + ' Confidence : ' + conf + ' Lift : ' + lift + '\\n'\n",
    "    i = i+1\n",
    "    f.write('%s\\n' % fstr)\n",
    "    print(l1, '->', l2)\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
